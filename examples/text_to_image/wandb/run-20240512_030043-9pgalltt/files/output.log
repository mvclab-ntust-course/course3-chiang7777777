05/12/2024 03:00:45 - INFO - __main__ - ***** Running training *****
05/12/2024 03:00:45 - INFO - __main__ -   Num examples = 33
05/12/2024 03:00:45 - INFO - __main__ -   Num Epochs = 1667
05/12/2024 03:00:45 - INFO - __main__ -   Instantaneous batch size per device = 1
05/12/2024 03:00:45 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
05/12/2024 03:00:45 - INFO - __main__ -   Gradient Accumulation steps = 4
05/12/2024 03:00:45 - INFO - __main__ -   Total optimization steps = 15000
Steps:   0%|                                                                                                                               | 0/15000 [00:01<?, ?it/s, lr=0.0001, step_loss=0.375]Traceback (most recent call last):
  File "/home/chiang777/Lab_hw/W3/diffusers/examples/text_to_image/train_text_to_image_lora.py", line 992, in <module>
    main()
  File "/home/chiang777/Lab_hw/W3/diffusers/examples/text_to_image/train_text_to_image_lora.py", line 809, in main
    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
  File "/home/chiang777/anaconda3/envs/diffusion_lora/lib/python3.12/site-packages/accelerate/accelerator.py", line 2269, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/chiang777/anaconda3/envs/diffusion_lora/lib/python3.12/site-packages/accelerate/accelerator.py", line 2219, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/chiang777/anaconda3/envs/diffusion_lora/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 337, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
                                              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/chiang777/anaconda3/envs/diffusion_lora/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 259, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.